{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of samples: 113\nThe number of columns: 160\n"
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "from hn.load_data import load_data\n",
    "#from brats.load_data import load_data\n",
    "#from hn.load_data import load_data\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "if data.isnull().values.any():\n",
    "    print('In the csv data file, some values are missing or NaN'), sys.exit\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of high risk patients: 55\nNumber of low risk patients: 58\n(90, 159) (90,)\n"
    }
   ],
   "source": [
    "features = data.loc[:, data.columns != 'label'].values\n",
    "labels = data.loc[:,['label']].values\n",
    "labels = [item if item!='T12' else 0 for item in labels]\n",
    "labels = [item if item!='T34' else 1 for item in labels]\n",
    "labels = np.array(labels)\n",
    "print(f'Number of high risk patients: {np.count_nonzero(labels)}') \n",
    "print(f'Number of low risk patients: {len(labels) - np.count_nonzero(labels)}')\n",
    "\n",
    "def split_sets(features, labels):\n",
    "    \"\"\"\n",
    "    splits the features and labels into a training set (80%) and test set (20%)\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=1)\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "x_train, x_test, y_train, y_test = split_sets(features, labels) \n",
    "print(x_train.shape, y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.6777777777777778\n"
    }
   ],
   "source": [
    "def leave_one_out_val(x,y):\n",
    "    \"\"\"\n",
    "    Leave One Out Cross Validation using Logistic Regression as a classifier\n",
    "    \"\"\"\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(x,y)\n",
    "\n",
    "    LeaveOneOut() \n",
    "\n",
    "    prediction = [] \n",
    "    y_val_total = []\n",
    "\n",
    "    for train_index, val_index in loo.split(x,y):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val= y[train_index], y[val_index]\n",
    "    \n",
    "        lrg= sklearn.linear_model.LogisticRegression()\n",
    "        lrg.fit(x_train,y_train) \n",
    "    \n",
    "        lrg_predicted=lrg.predict(x_val)\n",
    "        prediction.append(lrg_predicted)\n",
    "        y_val_total.append(y_val)\n",
    "    accuracy = accuracy_score(y_val_total, prediction)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = leave_one_out_val(x_train,y_train)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.6111111111111112, 0.6111111111111112, 0.8333333333333334, 0.5555555555555556, 0.6111111111111112]\n"
    }
   ],
   "source": [
    "def cross_val(x,y):\n",
    "    \"\"\"\n",
    "    Cross validation using a Logistic Regression classifier (5 folds)\n",
    "    \"\"\"\n",
    "\n",
    "    ss = StratifiedShuffleSplit(n_splits=5, test_size=0.2)            \n",
    "    ss.get_n_splits(x, y)\n",
    "\n",
    "    performances = [] \n",
    "\n",
    "    for train_index, val_index in ss.split(x, y):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val= y[train_index], y[val_index]\n",
    "\n",
    "        lrg=sklearn.linear_model.LogisticRegression()\n",
    "        lrg.fit(x_train,y_train) \n",
    "        prediction=lrg.predict(x_val)\n",
    "        accuracy = accuracy_score(y_val, prediction)\n",
    "        performances.append(accuracy)\n",
    "\n",
    "    return performances\n",
    "\n",
    "accuracy = cross_val(x_train, y_train)\n",
    "print(accuracy)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}