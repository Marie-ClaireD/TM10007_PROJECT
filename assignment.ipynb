{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template Head and Neck cancer Radiomics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.linear_model \n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of samples: 113\nThe number of columns: 160\n"
    }
   ],
   "source": [
    "from hn.load_data import load_data\n",
    "\n",
    "data = load_data()\n",
    "print(f'The number of samples: {len(data.index)}')\n",
    "print(f'The number of columns: {len(data.columns)}')\n",
    "\n",
    "if data.isnull().values.any():\n",
    "    print('In the csv data file, some values are missing or NaN'), sys.exit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing \n",
    "The data is split into feature values and labels. (high risk or low risk). The amount of high-risk and low-risk patients in printed as an output. Handling missing data and implementing standard scalar on the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of high risk patients: 55\nNumber of low risk patients: 58\n"
    }
   ],
   "source": [
    "features = data.loc[:, data.columns != 'label'].values\n",
    "features = StandardScaler().fit_transform(features)\n",
    "labels = data.loc[:,['label']].values\n",
    "labels = [item if item!='T12' else 0 for item in labels]\n",
    "labels = [item if item!='T34' else 1 for item in labels]\n",
    "labels = np.array(labels)\n",
    "print(f'Number of high risk patients: {np.count_nonzero(labels)}') \n",
    "print(f'Number of low risk patients: {len(labels) - np.count_nonzero(labels)}')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier moet feature selection komen, weet alleen nog niet helemaal hoe..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data mining \n",
    "The data is divided into a training, validation and test set. The validation set is used to optimize the hyperparameters during the data mining process. \n",
    "Two different classifiers will be used: Random Forest and k-nearest neighbour. A principal component analysis is computed. The amount of components to reach a 98% variance was calculated and used in the analysis. \n",
    "- Leave one out cross validation\n",
    "- KFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Leave one out cross validation accuracy: 0.6666666666666666\nKFold cross validation accuracies: [0.7222222222222222, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.7222222222222222, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.6111111111111112, 0.7222222222222222, 0.8333333333333334, 0.7222222222222222, 0.7222222222222222, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.7222222222222222, 0.6111111111111112, 0.5555555555555556, 0.5555555555555556, 0.6111111111111112, 0.6111111111111112, 0.6111111111111112, 0.6666666666666666, 0.6666666666666666, 0.7222222222222222, 0.9444444444444444, 0.5555555555555556, 0.7222222222222222, 0.5555555555555556, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.5, 0.6666666666666666, 0.8333333333333334, 0.6111111111111112, 0.5555555555555556, 0.6111111111111112, 0.5555555555555556, 0.6666666666666666, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.6111111111111112, 0.6666666666666666]\nKfold cross validation average accuracy: 0.6688888888888889\n"
    }
   ],
   "source": [
    "# splitting the data in a training and a test set \n",
    "def split_sets(features, labels):\n",
    "    \"\"\"\n",
    "    splits the features and labels into a training set (80%) and test set (20%)\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=1)\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "x_train, x_test, y_train, y_test = split_sets(features, labels) \n",
    "\n",
    "#The training set is again divided into a training and a validation set and afterwards classified using a leave one out validation and logistic regression.\n",
    "def leave_one_out_val(x,y):\n",
    "    \"\"\"\n",
    "    Leave One Out Cross Validation using Logistic Regression as a classifier\n",
    "    \"\"\"\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(x,y)\n",
    "\n",
    "    LeaveOneOut() \n",
    "\n",
    "    prediction = [] \n",
    "    y_val_total = []\n",
    "\n",
    "    for train_index, val_index in loo.split(x,y):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val= y[train_index], y[val_index]\n",
    "    \n",
    "        lrg= sklearn.linear_model.LogisticRegression()\n",
    "        lrg.fit(x_train,y_train) \n",
    "    \n",
    "        lrg_predicted=lrg.predict(x_val)\n",
    "        prediction.append(lrg_predicted)\n",
    "        y_val_total.append(y_val)\n",
    "    accuracy = accuracy_score(y_val_total, prediction)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = leave_one_out_val(x_train,y_train)\n",
    "print(f'Leave one out cross validation accuracy: {accuracy}')\n",
    "\n",
    "#The training set is again divided into a training and a validation set and afterwards classified using a Kfold cross validation and logistic regression.\n",
    "def cross_val(x,y):\n",
    "    \"\"\"\n",
    "    Cross validation using a Logistic Regression classifier (5 folds)\n",
    "    \"\"\"\n",
    "\n",
    "    crss_val = RepeatedKFold(n_splits = 5, n_repeats=10, random_state = None)           \n",
    "    crss_val.get_n_splits(x, y)\n",
    "\n",
    "    performances = [] \n",
    "\n",
    "    for train_index, val_index in crss_val.split(x, y):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val= y[train_index], y[val_index]\n",
    "\n",
    "        lrg=sklearn.linear_model.LogisticRegression()\n",
    "        lrg.fit(x_train,y_train) \n",
    "        prediction=lrg.predict(x_val)\n",
    "        accuracy = accuracy_score(y_val, prediction)\n",
    "        performances.append(accuracy)\n",
    "\n",
    "    return performances\n",
    "\n",
    "accuracy = cross_val(x_train, y_train)\n",
    "print(f'KFold cross validation accuracies: {accuracy}')\n",
    "print(f'Kfold cross validation average accuracy: {statistics.mean(accuracy)}')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "Outcome measures including: \n",
    "- boxplots\n",
    "- confusion matrix \n",
    "- ROC curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}