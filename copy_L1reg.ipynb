{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment Head and Neck cancer Radiomics\n",
    "\n",
    "Marie-Claire Doornbos ()\n",
    "Julia Holdorp (4561651)\n",
    "Quinten Mank (4336615)\n",
    "Ylva Weeda (4556038)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from scipy import interp\n",
    "from hn.load_data import load_data\n",
    "from scipy.stats import randint\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut, RepeatedKFold, StratifiedKFold \n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso, LogisticRegression, LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc, roc_curve, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data and preprocessing\n",
    "The Head and Neck cancer dataset is loaded into the script. If the dataset contains any missing values or NaN, the missing value is replaced with the mean of the feature in question. Eventually, the data is split into feature values and labels (high risk (1) or low risk (0)). The amount of high-risk and low-risk patients is printed as an output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The number of samples: 113\nThe number of columns: 160\n"
    }
   ],
   "source": [
    "\n",
    "from hn.load_data import load_data\n",
    "\n",
    "def load_check_data():\n",
    "    '''\n",
    "    Check if the datafile exists and is valid before reading\n",
    "    '''\n",
    "    # Check whether datafile exists\n",
    "    try:\n",
    "        data = load_data()\n",
    "        print(f'The number of samples: {len(data.index)}')\n",
    "        print(f'The number of columns: {len(data.columns)}')\n",
    "    except FileNotFoundError:\n",
    "        return print(\"The csv datafile does not exist\"), sys.exit()\n",
    "    except pd.errors.ParserError:\n",
    "        return print('The csv datafile is not a proper csv format.'\n",
    "                     'Please provide a data file in csv format.'), sys.exit()\n",
    "    # Check whether data is missing.\n",
    "    # If any datapoints are missing or NaN, these empty cells are replaced with the average \n",
    "    # of that feature.\n",
    "    if data.isnull().values.any():\n",
    "        column_mean = data.mean()\n",
    "        data = data.fillna(column_mean)\n",
    "        print('In the csv data file, some values are missing or NaN.'\n",
    "              ' These missing values are replaced by the mean of that feature.')\n",
    "    return data\n",
    "data = load_check_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Extract feature values and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of high risk patients: 55\nNumber of low risk patients: 58\n"
    }
   ],
   "source": [
    "# Features from data\n",
    "features = data.loc[:, data.columns != 'label'].values\n",
    "\n",
    "# Labels from data\n",
    "labels = data.loc[:,['label']].values\n",
    "#low risk patients receive the label 0 and high risk the label 1\n",
    "labels = [item if item!='T12' else 0 for item in labels]\n",
    "labels = [item if item!='T34' else 1 for item in labels]\n",
    "labels = np.array(labels)\n",
    "#number of high and low risk patients is printed to the terminal\n",
    "print(f'Number of high risk patients: {np.count_nonzero(labels)}') \n",
    "print(f'Number of low risk patients: {len(labels) - np.count_nonzero(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature selection\n",
    "Two different feature selection/extraction tools will be used. Firstly, least absolute shrinkage and selection operator (LASSO) will be executed to find the most distinctive and informative features for analysis. Later on, a principal component analysis (PCA) is executed while training the model. The amount of principal components were calculated beforehand. In the end, the two different methods will be compared to see which method works best!  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 LASSO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Principal components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters optimalization\n",
    "In this part, the data is seperated into a training and a test set. A standard scaler is used to standardize the training set. The training set is fitted and transformed, while the test set is only transformed. (never touch your test set!!) The following classifiers will be used: Logistic Regression, KNN, Random Forest and SVM. In order to find the optimal hyperparameters, a random grid search is executed. The optimal parameters are printed to the terminal and are used for validation. Two different validation methods were used: Leave one out cross validation and repeated KFold cross validation (5 folds and .. repeats). Inside this cross validation, the training set is once again splitted into a training set and a validation set. The performance is determined by the accuracy, sensitivity, specificity and auc. Also, ROC plots are plotted for the different classifiers.  \n",
    "- ook nog even de learning curve in verwerken! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Splitting the data in training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(features, labels):\n",
    "    \"\"\"\n",
    "    Splits the features and labels into a training set (80%) and test set (20%)\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=None)\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test \n",
    "\n",
    "x_train, x_test, y_train, y_test = split_sets(features, labels) \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Getting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n{'algorithm': 'auto',\n 'leaf_size': 30,\n 'metric': 'minkowski',\n 'metric_params': None,\n 'n_jobs': None,\n 'n_neighbors': 6,\n 'p': 2,\n 'weights': 'uniform'}\n/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n{'bootstrap': True,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': 4,\n 'max_features': 11,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 15,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 175,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "C <= 0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 516, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\", line 209, in fit\n    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\", line 268, in _dense_fit\n    max_iter=self.max_iter, random_seed=random_seed)\n  File \"sklearn/svm/libsvm.pyx\", line 186, in sklearn.svm.libsvm.fit\nValueError: C <= 0\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9fa39ab1f904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhyperparameters_clsfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-9fa39ab1f904>\u001b[0m in \u001b[0;36mget_hyperparameters\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclsfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: C <= 0"
     ]
    }
   ],
   "source": [
    "def get_hyperparameters(x, y):\n",
    "    \"\"\" \n",
    "    Random Search for Hyperparameters classifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    clsfs = [KNeighborsClassifier(), RandomForestClassifier(bootstrap=True, random_state=None), SVC(probability=True)]\n",
    "    param_distributions = [{\"n_neighbors\": randint(1, 20)}, {\"n_estimators\": randint(1, 200),\n",
    "                \"max_features\": randint(5, 30),\n",
    "                \"max_depth\": randint(2, 18),\n",
    "                \"min_samples_leaf\": randint(1, 17)},{\"C\": randint(0.1, 100),\n",
    "                 \"gamma\": ['auto','scale'],\n",
    "                 \"kernel\": ['rbf','poly','sigmoid','linear']}]\n",
    "\n",
    "    hyperparameters_clsfs = []\n",
    "    for clf, param_dist in zip(clsfs, param_distributions):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=5, cv=5, n_jobs=-1)\n",
    "        model = random_search.fit(x, y)\n",
    "        parameters = model.best_estimator_.get_params()\n",
    "        pprint(parameters)\n",
    "        hyperparameters_clsfs.append(parameters)\n",
    "\n",
    "    return hyperparameters_clsfs\n",
    "\n",
    "hyperparameters = get_hyperparameters(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Leave one Out cross validation on training and validation set - PCA\n",
    "- Sensitivity, specificity, auc en accuracy even fixen in 1 boxplot\n",
    "- Alle ROC's in 1 plot ipv los\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-8df817698eb9>, line 40)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-8df817698eb9>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    x_val_selected.to_numpy()\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def leave_one_out_pca(x,y, hyperparameters, clf):\n",
    "    \"\"\"  \n",
    "    Leave One Out Cross Validation using Logistic Regression as a classifier\n",
    "    \"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(x,y)\n",
    "\n",
    "    accuracies = []\n",
    "    auc_scores = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "\n",
    "    predict_labels = [] \n",
    "    predict_proba =[]\n",
    "    y_val_total = []\n",
    "\n",
    "    for train_index, val_index in loo.split(x,y):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val= y[train_index], y[val_index]\n",
    "\n",
    "        lasso = SelectFromModel(estimator=Lasso(alpha=0.001, random_state=None, max_iter=10000), threshold='median')\n",
    "        lasso.fit(x_train, y_train) \n",
    "        lasso.get_support()\n",
    "        selected_feat = data.columns[(lasso.get_support())]        \n",
    "        \n",
    "        print('Total number of features: {}'.format((x_train.shape[1])))\n",
    "        print('Number of selected features with LASSO: {}'.format(len(selected_feat)))\n",
    "        print('Number of features with coefficients shrank to zero: {}'.format(\n",
    "             np.sum(lasso.estimator_.coef_ == 0)))\n",
    "        print('Feature coefficients:',lasso.estimator_.coef_)\n",
    "\n",
    "        # Getting a list of removed features\n",
    "        removed_feats = [(lasso.estimator_.coef_ == 0).ravel().tolist()]\n",
    "        # print(removed_feats)\n",
    "\n",
    "        # Remove features from training and validation set\n",
    "        x_train_selected = lasso.transform(pd.DataFrame(x_train).fillna(0))\n",
    "        x_train = x_train_selected.to_numpy()\n",
    "        x_val_selected = lasso.transform(pd.DataFrame(x_val).fillna(0)\n",
    "        x_val_selected.to_numpy()\n",
    "\n",
    "        clf.fit(x_train_selected,y_train) \n",
    "        clf_predicted=clf.predict(x_val_selected)\n",
    "        predict_labels.append(clf_predicted)\n",
    "        predict = clf.predict_proba(x_val_selected)[:,1]\n",
    "        predict_proba.append(predict)\n",
    "        y_val_total.append(y_val)\n",
    "    \n",
    "    performance_scores = pd.DataFrame() \n",
    "    predict_labels = np.array(predict_labels)\n",
    "    auc_scores.append(roc_auc_score(y_val_total, predict_labels))\n",
    "    conf_mat = confusion_matrix(y_val_total, predict_labels)\n",
    "    total = sum(sum(conf_mat))\n",
    "    accuracies.append((conf_mat[0, 0]+conf_mat[1, 1])/total)\n",
    "    sensitivities.append(conf_mat[0, 0]/(conf_mat[0, 0]+conf_mat[0, 1]))\n",
    "    specificities.append(conf_mat[1, 1]/(conf_mat[1, 0]+conf_mat[1, 1]))\n",
    "    performance_scores['Accuracy'] = accuracies\n",
    "    performance_scores['AUC'] = auc_scores\n",
    "    performance_scores['Sensitivity'] = sensitivities\n",
    "    performance_scores['Specificity'] = specificities\n",
    "\n",
    "    predict_proba = np.array(predict_proba)\n",
    "    fpr, tpr, __ = roc_curve(y_val_total, predict_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(1, figsize=(12,6))\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='LOOCV ROC (AUC = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return performance_scores\n",
    "\n",
    "performance_clf = []\n",
    "clsfs = [LogisticRegression(), KNeighborsClassifier(n_neighbors=hyperparameters[0].get('n_neighbors')), RandomForestClassifier(bootstrap=True, max_depth=hyperparameters[1].get('max_depth'), max_features=hyperparameters[1].get('max_features'), min_samples_leaf=hyperparameters[1].get('min_samples_leaf'), n_estimators=hyperparameters[1].get('n_estimators'), random_state=None), SVC(C=hyperparameters[2].get(\"C\"), gamma=hyperparameters[2].get(\"gamma\"), kernel=hyperparameters[2].get(\"kernel\"), probability=True)]\n",
    "clsfs_names =['Logistic Regression', 'kNN', 'Random Forest', 'SVM']\n",
    "\n",
    "for clf in clsfs:\n",
    "    performances = leave_one_out_pca(x_train, y_train, hyperparameters, clf) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y_val_total_loo' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4c876d4e279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mplot_roc_loo_pca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_total_loo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_proba_loo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_val_total_loo' is not defined"
     ]
    }
   ],
   "source": [
    "# ROC plot for leave one out cross validation \n",
    "def plot_roc_loo_pca(y_val, predicted_probas):\n",
    "    fpr, tpr, __ = roc_curve(y_val, predicted_probas)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(1, figsize=(12,6))\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.5, label='LOOCV ROC (AUC = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=.8)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_loo_pca(y_val_total_loo, predict_proba_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Repeated KFold cross validation on training and validation set - PCA\n",
    "- ROC's in 1 grafiek\n",
    "- boxplots in 1 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-657d17f3094b>, line 43)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-657d17f3094b>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    clf.fit(x_train_selected, y_train)\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def cross_val_scores_pca(x, y, hyperparameters, clf):\n",
    "    '''\n",
    "    Cross validation using a Logistic Regression classifier (5 folds)\n",
    "    '''\n",
    "    \n",
    "    crss_val = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
    "    crss_val.get_n_splits(x, y)\n",
    "\n",
    "    accuracies = []\n",
    "    auc_scores = []\n",
    "    specificities = []\n",
    "    sensitivities = []\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(pd.DataFrame(x_train).fillna(0))\n",
    "\n",
    "    for train_index, val_index in crss_val.split(x, y):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val= y[train_index], y[val_index]\n",
    "\n",
    "        lasso = SelectFromModel(estimator=Lasso(alpha=0.001, random_state=None, max_iter=10000), threshold='median')\n",
    "        lasso.fit(scaler.transform(pd.DataFrame(x_train).fillna(0)), y_train) \n",
    "        lasso.get_support()\n",
    "        selected_feat = data.columns[(lasso.get_support())]        \n",
    "        \n",
    "        print('Total number of features: {}'.format((x_train.shape[1])))\n",
    "        print('Number of selected features with LASSO: {}'.format(len(selected_feat)))\n",
    "        print('Number of features with coefficients shrank to zero: {}'.format(\n",
    "             np.sum(lasso.estimator_.coef_ == 0)))\n",
    "        print('Feature coefficients:',lasso.estimator_.coef_)\n",
    "\n",
    "        # Getting a list of removed features\n",
    "        removed_feats = [(lasso.estimator_.coef_ == 0).ravel().tolist()]\n",
    "        # print(removed_feats)\n",
    "\n",
    "        # Remove features from training and validation set\n",
    "        x_train_selected = lasso.transform(pd.DataFrame(x_train).fillna(0))\n",
    "        x_val_selected = lasso.transform(pd.DataFrame(x_val).fillna(0)\n",
    "            \n",
    "        clf.fit(x_train_selected, y_train)\n",
    "        prediction = clf.predict(x_val_selected)\n",
    "\n",
    "        performance_scores = pd.DataFrame()                  \n",
    "        auc_scores.append(roc_auc_score(y_val, prediction))\n",
    "        conf_mat = confusion_matrix(y_val, prediction)\n",
    "        total = sum(sum(conf_mat))\n",
    "        accuracies.append((conf_mat[0, 0]+conf_mat[1, 1])/total)\n",
    "        sensitivities.append(conf_mat[0, 0]/(conf_mat[0, 0]+conf_mat[0, 1]))\n",
    "        specificities.append(conf_mat[1, 1]/(conf_mat[1, 0]+conf_mat[1, 1]))\n",
    "        performance_scores['Accuracy'] = accuracies\n",
    "        performance_scores['AUC'] = auc_scores\n",
    "        performance_scores['Sensitivity'] = sensitivities\n",
    "        performance_scores['Specificity'] = specificities\n",
    "\n",
    "        predicted_probas = clf.predict_proba(x_val)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_val, predicted_probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    " \n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    mean_auc = auc(base_fpr, mean_tprs)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(base_fpr, mean_tprs, 'c', alpha=0.8, label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),)\n",
    "    plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='c', alpha=0.2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Chance level', alpha=0.8)\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(f'Receiver operating characteristic (ROC) curve {clf}')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return performance_scores\n",
    "\n",
    "performance_clf = []\n",
    "clsfs = [LogisticRegression(), KNeighborsClassifier(n_neighbors=hyperparameters[0].get('n_neighbors')), RandomForestClassifier(bootstrap=True, max_depth=hyperparameters[1].get('max_depth'), max_features=hyperparameters[1].get('max_features'), min_samples_leaf=hyperparameters[1].get('min_samples_leaf'), n_estimators=hyperparameters[1].get('n_estimators'), random_state=None), SVC(C=hyperparameters[2].get(\"C\"), gamma=hyperparameters[2].get(\"gamma\"), kernel=hyperparameters[2].get(\"kernel\"), probability=True)]\n",
    "clsfs_names =['Logistic Regression', 'kNN', 'Random Forest', 'SVM']\n",
    "\n",
    "for clf in clsfs:\n",
    "    performances = cross_val_scores_pca(x_train, y_train, hyperparameters, clf) \n",
    "    performance_clf.append(performances)\n",
    "\n",
    "for item in performance_clf: \n",
    "    plt.figure()\n",
    "    item.boxplot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Repeated KFold cross validation on training and validation set - LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6 LASSO vs. PCA\n",
    "- Analyse whether LASSO or PCA works better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Hyperparameters optimalisation vs. no optimalisation\n",
    "- Analyse whether the hyperparameter optimalisation works better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "To evaluate the machine learning algoritm for the prediction of the T-score in head and neck cancer patients, different outcome measures are used. The area under the ROC curve is plotted and also accuracy, sensitivity and specificity are computed using the confusion matrix. The evalution is only executed on the test set to see whether an accuracy of 70% is achieved or not. \n",
    "\n",
    "--> box plots\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}